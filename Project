# -*- coding: utf-8 -*-
"""5_22201301_22201289_14_CSE422_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UmfDTCRjLClpBbxHkYZdVXH4CKyJXgG3
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd


from sklearn.linear_model import LogisticRegression
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder
from sklearn.metrics import classification_report, accuracy_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay

from sklearn.neighbors import KNeighborsClassifier

from google.colab import files
uploaded_files = files.upload()
df = pd.read_csv('software_quality_dataset.csv')

"""1.

"""

df.head(5)

df.info()

print(df.shape)

numerical_data = df.select_dtypes(include=['int64', 'float64'])
numerical_features = numerical_data.columns.tolist()
print(f'\nThere are {len(numerical_features)} numerical features:', '\n')
print(numerical_features)

categorical_data = df.select_dtypes(include=['object'])
categorical_features = categorical_data.columns.tolist()
print(f'\nThere are {len(categorical_features)} categorical features:', '\n')
print(categorical_features)

numerical_data.describe().T

#Variance of numerical features
print("\nVariance of numerical features:")
print(numerical_data.var())

#Skewness of numerical features
print("\nSkewness of numerical features:")
print(numerical_data.skew())

#Histograms for numerical features
numerical_data.hist(figsize=(12, 12), bins=20)
plt.show()

#Box plots for numerical features
plt.figure(figsize=(20, 30))
numeric_cols = numerical_data.columns
for i, col in enumerate(numeric_cols, 1):
    plt.subplot(len(numeric_cols), 1, i)
    sns.boxplot(x=df[col], color='skyblue')
    plt.title(f'Boxplot of {col}', fontsize=12)
    plt.tight_layout()
plt.show()

#Number of unique values in numerical features
print("\nNumber of unique values in numerical features:")
print(numerical_data.nunique())

#Number of unique values in categorical features
print("\nNumber of unique values in categorical features:")
unique_counts = categorical_data.nunique()
print(unique_counts)

#Bar plots for categorical features
for col in categorical_features:
    plt.title(f'Distribution of {col}')
    categorical_data[col].value_counts().sort_index().plot(kind='bar', rot=0, xlabel=col, ylabel='count')
    plt.show()

categorical_data.describe().T

#Encode Quality_Label for correlation analysis
df['Quality_Label_Encoded'] = df['Quality_Label'].map({'High': 2, 'Medium': 1, 'Low': 0})

#Update numerical data to include encoded Quality_Label
numerical_data = df.select_dtypes(include=['int64', 'float64'])

#Correlation matrix
print("\nCorrelation matrix of numerical features:")
correlation_matrix = numerical_data.corr()
print(correlation_matrix)

#Heatmap of correlation matrix
plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.3f', linewidths=0.3)
plt.title('Correlation Heatmap of Software Quality Dataset')
plt.show()

#Correlation with target (Quality_Label_Encoded) using different methods
fig, ax = plt.subplots(3, 1, figsize=(10, 10))
corr1 = numerical_data.corr('pearson')[['Quality_Label_Encoded']].sort_values(by='Quality_Label_Encoded', ascending=False)
corr2 = numerical_data.corr('spearman')[['Quality_Label_Encoded']].sort_values(by='Quality_Label_Encoded', ascending=False)
corr3 = numerical_data.corr('kendall')[['Quality_Label_Encoded']].sort_values(by='Quality_Label_Encoded', ascending=False)

ax[0].set_title('Pearson method')
ax[1].set_title('Spearman method')
ax[2].set_title('Kendall method')

sns.heatmap(corr1, ax=ax[0], annot=True)
sns.heatmap(corr2, ax=ax[1], annot=True)
sns.heatmap(corr3, ax=ax[2], annot=True)

plt.show()

#IMBALANCE
#Check imbalance in Quality_Label
class_counts = df.groupby("Quality_Label").size()

columns = ['quality_label', 'count', 'percentage']
quality_labels = ['High', 'Medium', 'Low']
count = []
percentage = []

#Calculate counts and percentages
for val in quality_labels:
    count.append(class_counts[val])
    percent = (class_counts[val] / 1000) * 100
    percentage.append(percent)

#Create DataFrame
imbalance_df = pd.DataFrame(list(zip(quality_labels, count, percentage)), columns=columns)
print("\nClass imbalance summary:")
print(imbalance_df)

#Bar plot of Quality_Label vs Percentage
sns.barplot(data=imbalance_df, x=imbalance_df['quality_label'], y=imbalance_df['percentage'])
plt.title('Distribution of Quality_Label Classes')
plt.xlabel('Quality Label')
plt.ylabel('Percentage')
plt.show()

"""There is imbalance between distribution of quality label classes."""

#imputing
#Check missing values
print("Number of missing values per column:\n", df.isnull().sum())

#Select numeric columns for imputation
numeric_cols = ['Lines_of_Code', 'Cyclomatic_Complexity', 'Num_Functions',
                'Code_Churn', 'Comment_Density', 'Num_Bugs', 'Code_Owner_Experience']

#Initialize SimpleImputer with mean strategy
impute = SimpleImputer(missing_values=np.nan, strategy='mean')

#Fit and transform numeric columns
df[numeric_cols] = impute.fit_transform(df[numeric_cols])

#Verify no missing values remain
print("\nNumber of missing values after imputation:\n", df.isnull().sum())

#Encoding
#Encode Has_Unit_Tests
df['Has_Unit_Tests'] = df['Has_Unit_Tests'].map({'Yes': 1, 'No': 0})

#Encode Quality_Label
df['Quality_Label'] = df['Quality_Label'].map({'High': 2, 'Medium': 1, 'Low': 0})

#Verify encoding
print("\nUnique values in Has_Unit_Tests:", df['Has_Unit_Tests'].unique())
print("Unique values in Quality_Label:", df['Quality_Label'].unique())

#Scaling
#Select numeric features for scaling
features_to_scale = ['Lines_of_Code', 'Cyclomatic_Complexity', 'Num_Functions', 'Code_Churn', 'Comment_Density', 'Num_Bugs', 'Code_Owner_Experience']

#Initialize StandardScaler
scaler = StandardScaler()

#Fit and transform features
df[features_to_scale] = scaler.fit_transform(df[features_to_scale])

#Verify scaling
print("\nPer-feature minimum after scaling:\n", df[features_to_scale].min())
print("Per-feature maximum after scaling:\n", df[features_to_scale].max())

print("Missing values:", df.isnull().sum())
print("Quality_Label values:", df['Quality_Label'].unique())
print("Feature ranges:", df.describe())

#split train-test

#Define features and target
X = df[['Lines_of_Code', 'Cyclomatic_Complexity', 'Num_Functions','Code_Churn', 'Comment_Density', 'Num_Bugs', 'Has_Unit_Tests', 'Code_Owner_Experience']]
y = df['Quality_Label']

#Perform train-test split (70% training, 30% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

#Print shapes
print("Training set shape:", X_train.shape)
print("Test set shape:", X_test.shape)

#for knn
X = df[['Lines_of_Code', 'Cyclomatic_Complexity', 'Num_Functions','Code_Churn', 'Comment_Density', 'Num_Bugs', 'Has_Unit_Tests','Code_Owner_Experience']]
y = df['Quality_Label']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)


knn = KNeighborsClassifier(n_neighbors=5)


knn.fit(X_train, y_train)

#Make predictions on test set
y_pred = knn.predict(X_test)

#Evaluate accuracy
print("Test set predictions:\n", y_pred)
print("Test set score: {:.2f}".format(knn.score(X_test, y_test)))

# for logistic regression
X = df[['Lines_of_Code', 'Cyclomatic_Complexity', 'Num_Functions','Code_Churn', 'Comment_Density', 'Num_Bugs', 'Has_Unit_Tests','Code_Owner_Experience']]
y = df['Quality_Label']

#Perform train-test split (70% training, 30% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

#Initialize Logistic Regression model
lr = LogisticRegression(max_iter = 5000)

#Train the model
lr.fit(X_train, y_train)

#Make predictions on test set
y_pred = lr.predict(X_test)

#Evaluate accuracy
print("Test set predictions:\n", y_pred)
print("Test set score: {:.2f}".format(lr.score(X_test, y_test)))

#Neural Network

X = df[['Lines_of_Code', 'Cyclomatic_Complexity', 'Num_Functions',
             'Code_Churn', 'Comment_Density', 'Num_Bugs', 'Has_Unit_Tests',
             'Code_Owner_Experience']].values
y = df['Quality_Label'].values.reshape(-1, 1)

# Perform train-test split (70% training, 30% testing)
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

# Custom neural network implementation
class ReLU:
    def forwardPropagation(self, inp):
        self.inputAct = np.maximum(0, inp)
        return self.inputAct

    def backPropagation(self, delta):
        return delta * np.where(self.inputAct > 0, 1, 0)

class MeanSquareError:
    def loss(self, Y, y):
        self.Y = Y
        self.y = y
        return np.abs((Y - y) ** 2).sum() / y.shape[0]

    def deriv(self):
        return (self.Y - self.y) / self.y.shape[0]

class GradientDescent:
    def __init__(self, learningRate):
        self.learningRate = learningRate

    def gradients(self, gradients):
        return self.learningRate * gradients

class NeuronLayer:
    def __init__(self, inputNeurons, outputNeurons, activation, biasFlag=True, randomState=42):
        np.random.seed(randomState)
        self.inputNeurons = inputNeurons
        self.outputNeurons = outputNeurons
        self.biasFlag = biasFlag
        self.activation = activation()
        self.weights = np.random.uniform(-np.sqrt(2/(inputNeurons + outputNeurons)), np.sqrt(2/(inputNeurons + outputNeurons)), size=(inputNeurons, outputNeurons))
        self.bias = np.random.uniform(-np.sqrt(2/(inputNeurons + outputNeurons)), np.sqrt(2/(inputNeurons + outputNeurons)), size=(1, outputNeurons))

    def build(self, optimizer, learningRate):
        self.learningRate = learningRate
        self.optimizer = optimizer(learningRate)

    def forwardPropagation(self, X):
        self.X = X
        self.output = np.dot(self.X, self.weights) + (self.biasFlag * self.bias)
        self.output = self.activation.forwardPropagation(self.output)
        return self.output

    def backPropagation(self, upstreamGradient):
        delta = self.activation.backPropagation(upstreamGradient)
        weightGrad = np.dot(self.X.T, delta) / self.X.shape[0]
        biasGrad = np.dot(np.ones((1, self.X.shape[0])), delta) / self.X.shape[0]
        self.weights -= self.optimizer.gradients(weightGrad)
        self.bias -= self.optimizer.gradients(biasGrad)
        downstreamGradient = np.dot(delta, self.weights.T) / self.X.shape[0]
        return downstreamGradient

class Model:
    def __createBatch(self, X, Y, batchSize):
        miniX, miniY = np.array([X[:batchSize]]), np.array([Y[:batchSize]])
        for idx in range(1, X.shape[0] // batchSize):
            miniX = np.append(miniX, np.array([X[idx * batchSize : (idx + 1) * batchSize]]), axis=0)
            miniY = np.append(miniY, np.array([Y[idx * batchSize : (idx + 1) * batchSize]]), axis=0)
        return miniX, miniY

    def __forwardPropagation(self, X):
        output = X
        for layer in self.layers:
            output = layer.forwardPropagation(output)
        return output

    def __backPropagation(self, Y):
        gradient = Y
        for layer in self.layers[::-1]:
            gradient = layer.backPropagation(gradient)

    def layers(self, layers):
        self.layers = layers

    def compile(self, loss, optimizer, learningRate):
        self.loss = loss()
        for layer in self.layers[::-1]:
            layer.build(optimizer, learningRate)

    def predict(self, X):
        return self.__forwardPropagation(X)

    def evaluate(self, X, Y):
        output = self.predict(X)
        return self.loss.loss(output, Y)

    def fit(self, X, Y, epochs, batchSize=None):
        batchSize = batchSize if batchSize else X.shape[0]
        self.X, self.Y = self.__createBatch(X, Y, batchSize)
        self.error = np.array([])
        for epoch in range(epochs):
            epochError = np.array([])
            for idx in range(self.X.shape[0]):
                epochError = np.append(epochError, self.loss.loss(self.__forwardPropagation(self.X[idx]), self.Y[idx]))
                self.__backPropagation(self.loss.deriv())
            epochError /= epochError.shape[0]
            self.error = np.append(self.error, epochError.sum() / epochError.shape[0])
            print("Epoch:", epoch + 1, "Error:", round(epochError[0], 2))
        return self.error

# Create and compile the model
model = Model()
model.layers = [
    NeuronLayer(8, 64, ReLU),  # Input layer (8 features) to hidden layer (64 neurons)
    NeuronLayer(64, 64, ReLU), # Hidden layer to hidden layer
    NeuronLayer(64, 3, ReLU)   # Hidden layer to output layer (3 classes)
]
model.compile(loss=MeanSquareError, optimizer=GradientDescent, learningRate=0.01)

# Train the model
model.fit(X_train, y_train, epochs=100, batchSize=32)

# Test the model
y_pred_continuous = model.predict(X_test)
y_pred = np.round(y_pred_continuous).astype(int)  # Round to nearest integer (0, 1, 2)
y_pred = np.clip(y_pred, 0, 2)  # Ensure predictions are within [0, 2]

# Evaluate accuracy
accuracy = np.mean(y_pred == y_test)
print("Test set accuracy: {:.2f}".format(accuracy))

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_curve, roc_auc_score
#Train Models
# 1. KNN
knn_model = KNeighborsClassifier(n_neighbors=5)
knn_model.fit(X_train, y_train)
y_pred_knn = knn_model.predict(X_test)
y_pred_proba_knn = knn_model.predict_proba(X_test)

# 2. Logistic Regression
lr_model = LogisticRegression(max_iter=1000, random_state=1)
lr_model.fit(X_train, y_train)
y_pred_lr = lr_model.predict(X_test)
y_pred_proba_lr = lr_model.predict_proba(X_test)

# 3. Neural Network
nn_model = Model()
nn_model.layers = [
    NeuronLayer(8, 64, ReLU),
    NeuronLayer(64, 64, ReLU),
    NeuronLayer(64, 3, ReLU)
]
nn_model.compile(loss=MeanSquareError, optimizer=GradientDescent, learningRate=0.01)
nn_model.fit(X_train, y_train.reshape(-1, 1), epochs=100, batchSize=32)
nn_pred_continuous = nn_model.predict(X_test)
# Apply softmax to get probabilities for multiclass
exp_scores = np.exp(nn_pred_continuous - np.max(nn_pred_continuous, axis=1, keepdims=True))
y_pred_proba_nn = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)
y_pred_nn = np.argmax(y_pred_proba_nn, axis=1)

# Helper function to evaluate models (adapted for multiclass)
def evaluate_model(model_name, model, X_test, y_test, y_pred, y_pred_proba=None):
    print(f"--- {model_name} ---")
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, average='macro', zero_division=0)
    recall = recall_score(y_test, y_pred, average='macro', zero_division=0)
    f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)

    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision (macro): {precision:.4f}")
    print(f"Recall (macro): {recall:.4f}")
    print(f"F1-score (macro): {f1:.4f}")

    print("Classification Report:")
    print(classification_report(y_test, y_pred, target_names=['Class 0', 'Class 1', 'Class 2'], zero_division=0))

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2'], yticklabels=['Class 0', 'Class 1', 'Class 2'])
    plt.title(f'Confusion Matrix - {model_name}')
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.show()

    # ROC Curve and AUC Score (One-vs-Rest for multiclass)
    auc_scores = []
    if y_pred_proba is not None:
        plt.figure(figsize=(6,4))
        for i in range(3):  # For each class
            y_test_binary = (y_test == i).astype(int)
            fpr, tpr, _ = roc_curve(y_test_binary, y_pred_proba[:, i])
            auc_score = roc_auc_score(y_test_binary, y_pred_proba[:, i])
            auc_scores.append(auc_score)
            plt.plot(fpr, tpr, label=f'Class {i} (AUC = {auc_score:.2f})')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title(f'ROC Curve (One-vs-Rest) - {model_name}')
        plt.legend(loc='lower right')
        plt.show()
        avg_auc = np.mean(auc_scores)
        print(f"Average AUC Score (macro): {avg_auc:.4f}")
    else:
        avg_auc = np.nan
        print("AUC/ROC not available as probability scores were not provided.")

    return {'model': model_name, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1_score': f1, 'auc': avg_auc}


model_results = []
results_knn = evaluate_model("K-Nearest Neighbors", knn_model, X_test, y_test, y_pred_knn, y_pred_proba_knn)
model_results.append(results_knn)

results_lr = evaluate_model("Logistic Regression", lr_model, X_test, y_test, y_pred_lr, y_pred_proba_lr)
model_results.append(results_lr)

results_nn = evaluate_model("Neural Network", nn_model, X_test, y_test, y_pred_nn, y_pred_proba_nn)
model_results.append(results_nn)

# Summary
results_df = pd.DataFrame(model_results)
print("Model Performance Summary:")
print(results_df.sort_values(by='recall', ascending=False))

# Bar charts
# Accuracy
plt.figure(figsize=(10, 6))
sns.barplot(x='model', y='accuracy', data=results_df.sort_values('accuracy', ascending=False), palette='viridis')
plt.title('Model Accuracy Comparison')
plt.xlabel('Model')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.ylim(0, 1.0)
plt.show()

# Precision, Recall, F1-score, AUC
metrics_to_plot = ['precision', 'recall', 'f1_score', 'auc']
for metric in metrics_to_plot:
    plt.figure(figsize=(10, 6))
    sns.barplot(x='model', y=metric, data=results_df.sort_values(metric, ascending=False), palette='mako')
    plt.title(f'Model {metric.capitalize()} Comparison')
    plt.xlabel('Model')
    plt.ylabel(metric.capitalize())
    plt.xticks(rotation=45)
    plt.ylim(0, 1.0)
    plt.show()

